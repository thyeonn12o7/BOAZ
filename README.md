## Boaz Adv "Boyazer"
### 기업 연계 프로젝트 | with Vibe-On
#### 마케팅 시각화 / 서비스 기획 / 분석 시스템 구축

---------------------

#### Position | Data & Web Engineer

#### ◎ 마케팅 리포트
1) 기획
> 에듀테크 기업인 Vibe-On 의 다양한 마케팅 지표들을 활용해 더 효과적인 마케팅 방안을 제안하기 위해 해당 프로젝트를 기획했다. 기업 외부인의 시각으로써 지금까지의 마케팅 성과를 추적, 비교함으로써 보이지 않았던 마케팅 사각지대를 발굴함을 목표로 했다.

2) 진행
> 활용가능한 모든 데이터를 활용해 프로젝트를 진행했다. 메타(Meta) 광고 지표 데이터, 바이브온 톡톡후기 데이터, 일별 로그인 횟수, 생기부 분석 횟수, 신규고객 및 접속 고객 등 다양한 데이터를 기반으로 외부인의 시각에서 데이터에 대한 객관적인 접근을 통해 더 나은 마케팅 방법에 대한 제안으로 이어질 수 있도록 프로젝트를 구성했다.

3) 결과
> 데이터에 따르면 바이브온의 두가지 퍼소나는 학생과 학부모로 나뉘어지며 이 두 집단의 특성은 매우 상이했다. 광고 매체에 대한 민감도가 다르게 나타나고 계절효과에 따른 민감도 또한 달랐다. 그 중에서도 학생이 더 우세한 사용자 집단이었고, 학생들의 서비스 이용경험에 대한 추가적인 분석이 필요했다. 결과적으로 학생 고객층의 유입이 높은 주말과 신학기 및 입시철을 중심으로 인스타그램, 인플루언서 광고 등을 통해 학생 사용자의 유입을 더욱 극대화 시켜야한다는 결론으로 이어졌다.


#### ◎ 모의지원 100% 활용하기
1) 기획
> 현재 VibeOn 에서 제공하는 서비스 중 하나인 ‘모의지원 DATA’ 페이지는 유의미한 정보를 제공하고 있으나 사용자가 이를 한 눈에 파악하기 어려워 활용이 쉽지 않다고 파악했다. 따라서, 해당 정보를 효과적인 시각화를 통해 사용자가 편리하게 데이터를 찾고, 활용할 수 있도록 구성할 필요가 있기에 본 프로젝트를 기획했다. 진행에 앞서 실제 고등학생들을 대상으로 대입 시 가장 중요하게 여기는 조건 혹은 가장 필요한 정보 등에 대한 설문조사를 통해 크게 네 가지 파트로 나누어 페이지를 구성 및 기획했다.

2) 진행 및 결과
> ‘목표대학 & 전공검색’ | 사용자의 입력 값에 따라 해당 학교 및 모집 단위의 평균 내신과 생기부 원점수의 합격 범위에 대한 정보를 박스플랏을 활용해 효과적으로 시각화한다.
>
> ‘진단 검색' | 목표 대학의 선택된 소계열에 대한 진단 평가를 제공하고, 안정/적정/소신/도전/위험 다섯 개의 카테고리로 분류해 진단한다.
>
> ‘계열 별 학과설정' | 사용자가 입력한 목표 대학 및 모집 단위가 속한 소계열 정보를 제공하고 해당 소계열 내 모집 단위의 점수 분포도 함께 제공한다.
>
> ‘동내신대 경쟁력' | 비슷한 내신 등급의 학생들이 관심있어 하는 TOP10 대학을 소개한 후 동내신대 학생들의 생기부 점수 분포, 생기부 개인기록 비율, 리더십 내용에 대한 정보를 함께 나타낸다. 또한 각 대학 별 모의지원한 학생들의 데이터를 기반으로 전체 지원자 생기부 원점수와 생기부 개인기록 비율을 확인할 수 있도록 각 대학 별 상세정보 탭도 함께 제공한다.


#### ◎ 학생의 생기부에 생기를 더하다 VitalOn
1) 기획
> 대입 자소서 폐지로 인해 고등학교 생활기록부의 중요성은 더욱 크게 증가했다. 이는 곧 학생의 대학 입시에 선생님의 생활기록부 작성 역량이 중요한 영향을 미친다는 것을 의미한다. 이로 인해 각 교육청은 선생님들의 학교 생활기록부 역량 제고 연수 등을 통해 생활기록부의 질을 높이려고 하고 있으나 지역 간 격차가 매우 크고, 같은 지역 내에서도 학교의 특성에 따라 한계가 존재한다. 따라서, 언어 생성 모델을 활용해 선생님의 생활기록부 작성에 도움을 주어 이 격차를 해소할 수 있도록 하기 위해 이 프로젝트를 기획했다.

2) 진행
> 대형언어모델 중 언어 생성에 가장 효과적인 Decoder-Only 구조의 GPT-2와 GPT-3을 활용했다. 하지만 이들은 대형 언어모델로서, 언어의 구조와 의미를 인지하기 위하여는 매우 큰 규모의 코퍼스를 학습시켜야 하기 때문에 미리 Pretrained된 모델을 Fine-tuning하는 방식으로 접근 및 진행했다. 바이브온에서 제공한 우수 생활기록부 102만개의 문장을 GPT2-small(LASSL), GPT2(SKT), GPT3-small(LMKor) 모델에 파인튜닝(Fine-tuning)했다.

3) 결과
> 세개의 모델 모두 1점대의 퍼플렉시티(Perplexity)를 보이며 우수한 생활기록부 문장 생성 능력을 보였다. 더 큰 모델일수록 더 우수한 문장을 생성하는 것을 확인할 수 있었으나, 100mb급 모델인 GPT-2-small도 굉장히 낮은 loss를 지님으로써 적은 컴퓨팅 자원 하에서도 효과적으로 활용될 수 있는 가능성을 보였다. 해당 모델들은 보안 문제로 인해 오픈소스로 공개되지는 못하였으나, (주)바이브온에게 제공됨으로써 향후 서비스에 활용될 수 있다.


##### 최종 컨퍼런스 보고서 | 
##### Boyazer 팀 웹 페이지 | https://boyazer.netlify.app

---------------------
